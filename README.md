# Motivation

- simulate schema evolution on local machine
- Getting familiar with AVRO and avro generation
- Feeding schema from producer side and let consumers use it with right configs
- learn basic principles of schemas and schema registry
- get familiar with AVRO and JSON schema formats, it's compatibility rules
- evaluate libraries applicability and maturity

# TODO List

- Schema management of different messages in same topic
- validating message against schema on consumer side
- validating message on server

# Additional Reads

**Why compatibility is important, message size, throughput and more:**

https://medium.com/race-conditions/kafka-the-afterthoughts-the-schema-management-7ea30e9518e4

**Why schemas are important:**

https://www.confluent.io/blog/schema-registry-kafka-stream-processing-yes-virginia-you-really-need-one/

**Avro, Kafka and the Schema Registry:**

https://davidhettler.net/blog/avro-kafka-schema-registry/

**Support of JSON and Protobuf:**

https://www.confluent.io/blog/confluent-platform-now-supports-protobuf-json-schema-custom-formats/

**Schemas are contracts:**

https://www.confluent.io/blog/schemas-contracts-compatibility

**Schema evolution in Avro, Protocol Buffers and Thrift:**

https://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html

**Overview of different serialization strategies from Event Sourcing perspective:**

https://blog.softwaremill.com/the-best-serialization-strategy-for-event-sourcing-9321c299632b
